{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to ONNX\n",
    "\n",
    "- **Open Neural Network eXchange (ONNX)** is an open standard format for representing machine learning models.\n",
    "\n",
    "- The `torch.onnx` module provides APIs to **`capture the computation graph from a native PyTorch torch.nn.Module model and convert it into an ONNX graph`**.\n",
    "\n",
    "- The **`exported model can be consumed by any of the many runtime that support ONNX, including Microsoft’s ONNX Runtime`**.\n",
    "\n",
    "---\n",
    "\n",
    "## Two flavors of ONNX in PyTorch:\n",
    "\n",
    "1. **TorchDynamo-based ONNX Exporter**\n",
    "   - Supported in PyTorch 2.0 and later\n",
    "   - TorchDynamo engine is leveraged to hook into Python’s frame evaluation API and dynamically rewrite its bytecode into an FX Graph. The resulting FX Graph is then polished before it is finally translated into an ONNX graph.\n",
    "\n",
    "    - The **`main advantage`** of this approach is that the FX graph is captured using bytecode analysis that **`preserves the dynamic nature of the model instead of using traditional static tracing techniques`**.\n",
    "\n",
    "2. **TorchScript-based ONNX Exporter**\n",
    "    -  available since PyTorch 1.2.0    \n",
    "    - TorchScript is leveraged to `trace (through torch.jit.trace()) the model and capture a static computation graph`.\n",
    "\n",
    "    - As a consequence, the resulting graph has a couple limitations:\n",
    "\n",
    "        - It **`does not record any control-flow, like if-statements or loops`**;\n",
    "\n",
    "        - **`Does not handle nuances between training and eval mode`**;\n",
    "\n",
    "        - Does not truly handle dynamic inputs\n",
    "\n",
    "    - As an attempt to support the static tracing limitations, the exporter also supports TorchScript scripting (through torch.jit.script()), which adds support for data-dependent control-flow, for example. However, `TorchScript itself is a subset of the Python language, so not all features in Python are supported, such as in-place operations`.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "- Obviously, the **`TorchDynamo-based ONNX Exporter`** is the **`preferred approach`** for exporting models to ONNX format, as it **`preserves the dynamic nature of the model`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Installation:\n",
    "\n",
    "```bash\n",
    "pip install --upgrade onnx onnxscript onnxruntime\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Check Versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__='2.2.0'\n",
      "onnxscript.__version__='0.1.0.dev20240223'\n",
      "onnxruntime.__version__='1.17.0'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"{torch.__version__=}\")\n",
    "\n",
    "import onnxscript\n",
    "\n",
    "print(f\"{onnxscript.__version__=}\")\n",
    "\n",
    "import onnxruntime\n",
    "\n",
    "print(f\"{onnxruntime.__version__=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sample `neural network` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Export the model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Deependu/Library/Python/3.9/lib/python/site-packages/torch/onnx/_internal/exporter.py:137: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch_model = MyModel()\n",
    "torch_input = torch.randn(1, 1, 32, 32)\n",
    "onnx_program = torch.onnx.dynamo_export(torch_model, torch_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Save the ONNX model in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_program.save(\"my_image_classifier.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Visualize the ONNX model graph using Netron\n",
    "\n",
    "https://netron.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Execute the ONNX model with ONNX Runtime\n",
    "\n",
    "- Install ONNX runtime\n",
    "```bash\n",
    "pip install --upgrade onnxruntime\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 1\n",
      "Sample input: (tensor([[[[-0.9046, -0.4413,  0.5450,  ..., -0.0977,  0.6759, -0.4582],\n",
      "          [-1.5342,  1.3502,  0.6754,  ..., -1.5576, -0.3840,  1.8964],\n",
      "          [-1.1224, -1.1482,  0.1003,  ...,  0.1244, -1.2301,  0.5959],\n",
      "          ...,\n",
      "          [-0.5470,  0.9113,  0.3746,  ..., -0.0066,  0.4829,  0.5727],\n",
      "          [-0.5766, -0.0918, -0.1301,  ...,  0.4595, -0.7187, -2.3188],\n",
      "          [ 0.4009,  0.5273, -2.4092,  ...,  0.0299,  0.3198, -0.7431]]]]),)\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "onnx_input = onnx_program.adapt_torch_inputs_to_onnx(torch_input)\n",
    "print(f\"Input length: {len(onnx_input)}\")\n",
    "print(f\"Sample input: {onnx_input}\")\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\n",
    "    \"./my_image_classifier.onnx\", providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return (\n",
    "        tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "    )\n",
    "\n",
    "\n",
    "onnxruntime_input = {\n",
    "    k.name: to_numpy(v) for k, v in zip(ort_session.get_inputs(), onnx_input)\n",
    "}\n",
    "\n",
    "onnxruntime_outputs = ort_session.run(None, onnxruntime_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Compare the PyTorch results with the ones from the ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch and ONNX Runtime output matched!\n",
      "Output length: 1\n",
      "Sample output: [array([[-0.00951168, -0.04558313,  0.07988013, -0.01976795,  0.0758441 ,\n",
      "        -0.12008841, -0.0436755 ,  0.052744  ,  0.1311065 ,  0.07726569]],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "torch_outputs = torch_model(torch_input)\n",
    "torch_outputs = onnx_program.adapt_torch_outputs_to_onnx(torch_outputs)\n",
    "\n",
    "assert len(torch_outputs) == len(onnxruntime_outputs)\n",
    "for torch_output, onnxruntime_output in zip(torch_outputs, onnxruntime_outputs):\n",
    "    torch.testing.assert_close(torch_output, torch.tensor(onnxruntime_output))\n",
    "\n",
    "print(\"PyTorch and ONNX Runtime output matched!\")\n",
    "print(f\"Output length: {len(onnxruntime_outputs)}\")\n",
    "print(f\"Sample output: {onnxruntime_outputs}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
